# 把 LLoRA 搬到离散矩阵：最精简但仍保正交的 SVD 训练配方


---

## 0. 记号

- `H ∈ ℝ^{M×N}`：给定信道矩阵
- `L`：想保留的奇异值/秩
- `f_net`、`g_net`：两组 **MLP**
    - `f_net(one_hot(i)) → ℝ^L` 产出列向量组 `F ∈ ℝ^{N×L}`
    - `g_net(one_hot(j)) → ℝ^L` 产出行向量组 `G ∈ ℝ^{M×L}`

（如果矩阵很小，这两张网可以直接换成待优化参数 `F`,`G`。）

---

## 1. 损失函数

### 1.1 LLoRA 主项（保持近似 SVD）

原文的  
`𝓛_main = -2 ⟨gℓ | H fℓ⟩ + Σ_{ℓ,ℓ'} ⟨fℓ|fℓ'⟩⟨gℓ|gℓ'⟩`  
改成离散求和后可向量化为

```
𝓛_main = -2·tr(Gᵀ H F) + tr(Fᵀ F · Gᵀ G)
```

取小批量索引即可近似整式求和。

### 1.2 正交性约束（最简单写法）

```
𝓛_orth = ‖FᵀF - I‖²_F + ‖GᵀG - I‖²_F
```

### 1.3 总损失

```
𝓛 = 𝓛_main + λ · 𝓛_orth
```

`λ≈1` 往往就够；可调到“两项梯度量级相当”。

---

## 2. 训练小抄（PyTorch 伪码）

```python
M, N, L = H.shape[0], H.shape[1], L
f_net = MLP(N, 128, L)  # 输入 One-Hot 长度 N
g_net = MLP(M, 128, L)

opt = torch.optim.Adam(list(f_net.parameters())+list(g_net.parameters()), 1e-3)

for step in range(max_steps):
    # --- 随机采样一小批行、列索引 ---
    idx_i = torch.randint(0, N, (BATCH,))
    idx_j = torch.randint(0, M, (BATCH,))
    
    onehot_i = torch.eye(N)[idx_i]          # (BATCH, N)
    onehot_j = torch.eye(M)[idx_j]          # (BATCH, M)
    
    F_i = f_net(onehot_i)                   # (BATCH, L)
    G_j = g_net(onehot_j)                   # (BATCH, L)
    
    # H 子矩阵
    H_sub = H[idx_j][:, idx_i]              # (BATCH, BATCH)
    
    # -- LLoRA 主项近似 --
    main = -2*torch.einsum('bl,bb,bl->', G_j, H_sub, F_i)
    gram_f = F_i.T @ F_i / BATCH
    gram_g = G_j.T @ G_j / BATCH
    main += torch.trace(gram_f @ gram_g)
    
    # -- 正交性约束 --
    orth = (gram_f - torch.eye(L)).pow(2).sum() + (gram_g - torch.eye(L)).pow(2).sum()
    
    loss = main + lam * orth
    loss.backward(); opt.step(); opt.zero_grad()
    
    # 可选：整批 QR 投影保持完美正交
    if step % epoch_end == 0:
        with torch.no_grad():
            F_full = f_net(torch.eye(N))
            G_full = g_net(torch.eye(M))
            F_full, _ = torch.linalg.qr(F_full)     # (N,L)
            G_full, _ = torch.linalg.qr(G_full)     # (M,L)
            f_net.fit_to_table(F_full)              # 自行实现：把网络权重回归到这些目标
            g_net.fit_to_table(G_full)
```

QR 投影一步到位消灭 `𝓛_orth`，然后下一轮再训；也可以二选一。

---

## 3. 提取 U, Σ, V

1. 用 **全索引 one-hot** 推理一次，得到  
    `F = f_net(I_N) ∈ ℝ^{N×L}`，`G = g_net(I_M) ∈ ℝ^{M×L}`
2. 列范数  
    `σ_ℓ = ‖F[:,ℓ]‖₂ * ‖G[:,ℓ]‖₂`
3. 标准化  
    `V[:,ℓ] = F[:,ℓ] / ‖F[:,ℓ]‖₂`  
    `U[:,ℓ] = G[:,ℓ] / ‖G[:,ℓ]‖₂`
4. 构成 `Σ = diag(σ₁, …, σ_L)`，即可得到 `H ≈ U Σ Vᵀ`。

---

### 小提示

1. 正交损失不必太大；一旦网络学会了，它会自动把列向量推成互正交。

祝顺利拆解您的信道矩阵!


------
## 1. 低秩参数 L 从哪里来？

L 决定你想保留多少对「奇异值＋左右奇异向量」。常见取法：

| 场景        | 选 L 的常用准则                                                                                                                                                                                                                 |
| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 噪声抑制 / 压缩 | 选到累计能量 ≥ τ（如 90 % or 99 %）：设 σ₁≥σ₂≥…，找到最小 L 使 ![formula](https://latex.codecogs.com/svg.image?%5Csum_%7B%E2%84%93=1%7D%5E%7BL%7D%5Csigma_%7B%E2%84%93%7D%5E%7B2%7D,/,%5Csum_%7Bk%7D%5Csigma_%7Bk%7D%5E%7B2%7D%5Cge%5Ctau) |
| 通信 MIMO   | 受天线数、并行流条数或功率预算约束，L = min(#TxRF, #RxRF)                                                                                                                                                                                   |
| 不确定       | 先设稍大 L (如 64、128)，训练后看 σ_ℓ 曲线；若尾部 σ 很小可再裁剪                                                                                                                                                                                |

如果你想**完整 SVD**，就设

```
L = min(M, N)
```

此时理论上 `GFᵀ` 可以完全重构 `H`，损失函数同样适用，只是再无“降秩”一说。

---

## 2. “低秩近似”到底意味着什么？

- **秩 ≤ L**：矩阵 `GFᵀ` 只能用 **L 个线性独立列/行** 就能生成全部行列。
- **不是稀疏**：它仍然是一个 M × N 的稠密矩阵，只是自由度从 M·N 缩到 L·(M+N)。
- **维度不变**：输入输出尺寸依旧是 M×N，只是有效信息活在 L 维子空间里。

可以把它想成用 L 条独立通信路径／主成分来刻画整个信道。

---

## 3. 为什么 U,V 正交 ⇒ Σ 严格对角？

SVD 定义：

```
H = U Σ Vᵀ
```

其中

- U, V 列向量两两正交（单位向量）
- Σ 只能在对角线上放奇异值，其余元素 **理论上为 0**。

在数值计算或梯度训练时，U,V 仅近似正交 ⇒ `Σ̂ = UᵀHV` 会出现极小的非对角残差。  
• 完全正交 ⇒ 非对角元素=0。  
• 近似正交 ⇒ 把非对角元素置 0，可恢复真正的 σ_ℓ。

---

## 4. 从 U,V 得 Σ 的做法

1. **正规公式（推荐，任何 L）**
    
    ```
    Σ = diag(Uᵀ H V)          # 取对角线
    ```
    
    仅需一次矩阵乘。
    

---

## 5. 小结一句话回答你的疑问

- 损失函数最小化的是 `‖H − GFᵀ‖_F²`，它 **自动挑出最大的 L 个奇异值**，把其余（较小）奇异值“砍掉”，因此叫低秩近似。
- 低秩并非矩阵里有很多零，而是自由度降低到 L。
- 想完整 SVD 就把 L 设到满秩；损失函数依然可用，只是失去了“压缩”意义。
- 当 U、V 完全正交时，`Σ = Uᵀ H V` 会是严格对角矩阵，也就是只有对角线上有元素。
-----
## 1 列向量是否真的会“单位正交”？

1. 损失本身
    
    ```
    𝓛 = ‖H − G Fᵀ‖²_F
    ```
    
    只要求 `GFᵀ` 重构 `H`，**不直接要求** `FᵀF = I`, `GᵀG = I`。  
    因此：
    
    - 如果你 **额外加了正交项** `‖FᵀF−I‖² + ‖GᵀG−I‖²`，或者
    - 每个 epoch 后显式做 **QR / Gram–Schmidt 投影**，  
        列向量才会在数值上变成单位正交。  
        否则只能说“趋向正交”，范数会被吸收到奇异值里。
2. 一旦你用 QR 投影，得到的矩阵我们记为
    
    ```
    V  = QR( F )   →  N×L   （列范数 = 1）
    U  = QR( G )   →  M×L
    ```
    
    此时它们确实已经满足  
    `VᵀV = I_L , UᵀU = I_L`。
    

---

## 2 SVD 的“符号不确定”是先天的

• 真正的 SVD 有如下自由度：

  对每个阶数 ℓ，若同时把

```
u_ℓ ← −u_ℓ ,     v_ℓ ← −v_ℓ
```

  则 `U Σ Vᵀ` 不变，因为 `(-u_ℓ) σ_ℓ (-v_ℓ)ᵀ = u_ℓ σ_ℓ v_ℓᵀ`。

• 换句话说，**列向量的符号由任何基于 ‖·‖、点积的损失都无法确定**，除非另加外部约束。

---

## 3 如何给 U、V 选“可控符号”

下列做法都是 **训练完** 再做一次轻量后处理，不影响重构精度。

### 3.1 约定“最大幅度元为正”

对每个奇异向量 `x = u_ℓ`（或 `v_ℓ`）：

```python
idx = torch.argmax(torch.abs(x))
if x[idx] < 0:
    x *= -1
```

优点：

- 不依赖额外数据，操作 O(M) / O(N)。  
    缺点：
- 如果两个元素幅值一样大，则符号仍可能在不同运行间翻转（极少见）。

### 3.2 让 Σ 始终正数、只翻 v

做一次

```
σ_ℓ = u_ℓᵀ H v_ℓ        # 可正可负
if σ_ℓ < 0:
    v_ℓ *= -1
    σ_ℓ = -σ_ℓ
```

此时

- `Σ` 保证对角线全正；
- `U` 不动，`V` 统一调整符号；
- 不会破坏正交与重构。

### 3.3 与“参考方向”对齐

若有某种先验/上次迭代结果 `u_ref`、`v_ref`，直接最大化相关系数：

```python
if torch.dot(u_ℓ, u_ref) < 0:
    u_ℓ *= -1
    v_ℓ *= -1
```

用于增量/在线更新时最稳。

---

## 4 一页纸 Workflow 回顾

```text
1. 训练：
   minimize 𝓛 = ‖H − G Fᵀ‖² + λ‖FᵀF−I‖² + λ‖GᵀG−I‖²
   或每 epoch 对 F,G 做 QR 投影

2. 推理：
   F ← f_net(I_N)         # N×L
   G ← g_net(I_M)         # M×L
   V,U ← QR(F), QR(G)

3. 奇异值：
   Σ = diag(Uᵀ H V)       # L×L

4. 符号后处理（任选一）：
   for ℓ:
       if Σ_{ℓℓ} < 0:   v_ℓ *= -1 ; Σ_{ℓℓ} = -Σ_{ℓℓ}
   # now Σ ≥ 0, U,V orthonormal, signs固定

5. 检验：
   rel_err = ‖H − UΣVᵀ‖_F / ‖H‖_F
```

---

### 关键句

- 损失只能锁定“方向”，锁不住“朝向”；
- 想让符号可复现，就在 **后处理** 阶段给每一对 `(u_ℓ,v_ℓ)` 立一个简单的符号规则即可。

------
## 0 · 先说结论

- 损失函数仍然写成  
    `𝓛 = ‖H − G Fᴴ‖²_F`（ᴴ=共轭转置）就能逼近 **复数 SVD**。
- 需要做的只是把所有 **转置 → 共轭转置**、  
    **点积 → 共轭点积**，其余步骤完全平行。
- “符号不确定”在复域升级为 **整列有任意相位 φ，|φ|=1**，后处理时固定相位即可。

下文给出三件事：

1. 数学改动一览
2. 代码层面怎么做（PyTorch/不支持复数的框架）
3. 如何消除相位不确定

---

## 1 · 复数版本要改的数学符号

|实数|复数|
|---|---|
|转置 `T`|共轭转置 `†` or `ᴴ`|
|点积 `aᵀb`|`aᴴb`|
|Frobenius 范数 `‖X‖² = Σ|Xᵢⱼ|
|正交 `QᵀQ=I`|**酉** `QᴴQ=I`|

**损失展开**（丢常数项）：

```
𝓛 = ‖H − G Fᴴ‖²_F
    = ‖H‖²_F − 2·Re(tr(Gᴴ H F)) + ‖FᵀF‖_F · ‖GᴴG‖_F
```

只需把原来的 `tr(GᵀHF)` 改成 `Re(tr(GᴴHF))`。

---

## 2 · 实现途径

### 2.1 PyTorch / JAX 等已支持复数

```python
H = torch.randn(M, N, dtype=torch.cfloat)  # 示例
...
# 网络输出 real+imag 两路后一行拼成复数
F = f_net(one_hot_cols)[:, :L] + 1j * f_net(one_hot_cols)[:, L:]
G = g_net(one_hot_rows)[:, :L] + 1j * g_net(one_hot_rows)[:, L:]

# 主项
main = -2*torch.real(torch.trace(G.conj().T @ H @ F))

# 正交约束
gram_f = F.conj().T @ F
gram_g = G.conj().T @ G
orth = ((gram_f - torch.eye(L, dtype=torch.cfloat)).abs()**2).sum() + \
       ((gram_g - torch.eye(L, dtype=torch.cfloat)).abs()**2).sum()
loss = main + lam*orth
```

QR/Gram–Schmidt 用 `torch.linalg.qr` 同样支持复数，得到的是酉矩阵。


## 3 · 相位（符号）不确定怎么定？

复域下自由度为 **每列乘 e^{iφ_ℓ}**。常见定相法：

1. “最大幅度元素相位归零”
    
    ```
    idx = argmax(|u_ℓ|)
    φ   = angle(u_ℓ[idx])
    u_ℓ *= e^{-iφ};  v_ℓ *= e^{-iφ}
    ```
    
    保 U,V 酉，Σ 不变。
    
2. “Σ 全正实”  
    令 `σ_ℓ = u_ℓᴴ H v_ℓ`，若 `σ_ℓ` 有相位
    
    ```
    φ = angle(σ_ℓ)
    v_ℓ *= e^{-iφ};     σ_ℓ = |σ_ℓ|
    ```
    
    全部奇异值变成非负实数，一般与数学软件输出一致。
    
3. “对准参考向量”  
    若有历史 `u_ref`，使 `angle(u_ℓᴴ u_ref) = 0`，常用于时序跟踪。
    

---

## 4 · 快速 Checklist

```
[ ] 损失内积全部加 conj()，转置→†                
[ ] 正交项替换为 GᴴG, FᴴF                       
[ ] QR / Gram–Schmidt 用复版                   
[ ] 提取 Σ = diag(Uᴴ H V)，取绝对值              
[ ] 训练后做一次列相位对齐                      
[ ] 验证 ‖H − UΣVᴴ‖ / ‖H‖ 误差                
```

做到这些，就已经把“复信道矩阵的完整/截断 SVD” 端到端搬到了你的网络训练里。

-----------

先试一下用bjorck正交化，然后也可以试一下直接惩罚项