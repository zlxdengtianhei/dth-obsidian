下面给出一份系统性的优化攻略，专门针对「信道矩阵典型秩仅 2–4」这一先验，结合 SVD 分解数学性质与深度网络实现细节，分四个层面阐述：矩阵本身的性质、SVD 计算相关的可利用特性、网络架构设计、数据预处理与损失策略。

--------------------------------------------------------------------
一、矩阵与 SVD 的数学性质——可利用的先验
--------------------------------------------------------------------
1. 极低秩 (r≈2–4)  
   • 64×64 矩阵中只有不到 6% 的自由度是真正有效的。  
   • 奇异值谱高度尖锐，后 60 多个 σ 基本为零或噪声。

2. σ 分布呈强对数衰减  
   • σ₁ : σ₂ : σ₃ ... ≈ 几十 : 几 : 1。  
   • 排名外 σ 基本落在噪声带内（≤1e-2）。

3. U,V 的子空间不唯一  
   • 当 σᵢ 重复或接近时，U、V 的列向量在对应子空间内可任意旋转。  
   • 网络学到的 U,V 只需张成正确子空间，而不必与 “真值” 完全对齐 → 可用子空间距离度量（Chordal、Grassmann）。

4. 重构误差主要由 σ、U,V 前 r 列决定  
   • 只要 Uᵣ Σᵣ Vᵣᵀ 近似对，就能达到极低 AE。

--------------------------------------------------------------------
二、SVD 相关可利用技巧
--------------------------------------------------------------------
1. **直接预测低秩分块**  
   与其预测 32×64 的 U、V，不如：
   ```
   k = 4  # 或 6
   直接输出 U∈ℂ^{B×k×64},  Σ∈ℝ^{B×k},  V∈ℂ^{B×k×64}
   ```
   优点：  
   • 参数、MAC 降低 8×；  
   • 训练稳定（梯度主要落在有效秩子空间）；  
   • 推理时不浪费容量预测无用 σ。

2. **σ Soft-Sort 或 Soft-Rank 约束**  
   • 令网络输出 raw_s ∈ ℝ^{k}，再经 `softplus` 保正，再用 `sort()` 强制单调递减。  
   • 亦可加 `σ_i ≥ σ_{i+1} + ε` 的 Margin Loss，保持谱排序，防止列交换抖动。

3. **子空间距离损失**  
   • 传统 `‖UᵀU-I‖²` 只是正交性，无法约束与 GT 子空间对齐。  
   • 改用 Chordal Distance：  
     ```
     L_sub = ‖ÛÛᴴ - U Uᴴ‖_F² + ‖V̂V̂ᴴ - V Vᴴ‖_F²
     ```
     只关注投影矩阵，忽略列内旋转，更符合低秩场景。

4. **双重重构**  
   • 用网络输出的 k 阶 SVD 得到粗重构 T₀；  
   • 只对残差做一个极小卷积 UNet（或线性层）补偿，输出 ΔT。  
   • ΔT 的能量理论上应 ≪ 1%，可用 L₁ or L₂ 强稀疏。

--------------------------------------------------------------------
三、网络架构层面
--------------------------------------------------------------------
1. **轻量 Global-only 架构**  
   - 既然矩阵秩极低，“全局相关性” > “局部细节”。  
   - 用极简 Tiny-ViT / MLP-Mixer（L=2，embed=64~96）即可捕获全局特征；ConvMixer 分支可直接裁剪。

2. **λ-可变秩预测头**  
   - 让模型预测一个 mask w∈[0,1]^k（或温度化 Gumbel Softmax）表示“该 σ 是否激活”。  
   - 推理时按 w>0.1 选取有效秩，可自适应 2-4。  
   - 训练端 L₁(w) 促稀疏，保证低秩。

3. **参数共享的双头**  
   - U,V 预测可共用同一干网络，只在末层线性投影拆头。  
   - 极低秩场景下不必用独立通路，减少过拟合。

4. **复数权重 or 分离实虚**  
   - PyTorch 2.1 仍不原生支持复数卷积，但线性层可用。  
   - 简便方法：把实、虚 concat 成 2C 通道，网络做实运算，最后再组合成复数；  
   - 若坚持复数线性，可在头部 `complex_linear` 做一次昇维再切回。

--------------------------------------------------------------------
四、数据预处理与训练策略
--------------------------------------------------------------------
1. **谱归一化**  
   - 先对每个样本做局部 Frobenius 归一化：T ← T / ‖T‖_F。  
   - 再做全局 z-score。能消除样本间功率差，让网络集中拟合方向而非幅值。

2. **低秩噪声增强**  
   - 在训练数据上加一个 λ·N，N 为 rank-1 或 rank-2 的随机矩阵，而非 i.i.d 高斯噪声。  
   - 迫使模型学会恢复低秩结构，对测试集更鲁棒。

3. **分阶段学习率**  
   - warm-up 5 ep → Cosine 到 ep30 → 固定极小 lr 再训练 20ep。  
   - 观察之前 25→50 轮失效：学习率衰减太慢，后 25 轮没有真正 fine-tune，可把 `eta_min` 改成 1e-7 并在 30 轮后改成 Plateau 调度。

4. **正则权重渐进**  
   - 低秩先验强，正交/稀疏/子空间损失最好 **线性上升**：  
     ```
     λ_t = λ_max · sigmoid((epoch - 5)/3)
     ```
   - 避免前期梯度被强约束扼杀，后期再“收网”。

5. **梯度累积 + 小 batch**  
   - 低秩场景只依赖少量特征，可用 batch=4, grad_acc=8，在 24GB GPU 下仍稳。

--------------------------------------------------------------------
五、可落地的最小改动清单（建议路线）
--------------------------------------------------------------------
1. 把 `k=32` 改为 `k=6`（或直接 4）；  
2. 删除 ConvMixer 分支，仅保留 Global Tiny-ViT；  
3. 在 `svd_head` 输出中加入 `softplus + sort` 处理 σ；  
4. 损失函数：
   ```
   L = L_MSE  +  λ1 · L_sparse(ΔT)  +  λ2 · L_chordal(U,V)
   λ1=1e-3 线性↑到 5e-3,  λ2=1e-2 线性↑到 5e-2
   ```
5. 训练 35 轮 (warm5 + cos20 + plateau10)，eta_min=1e-7；  
6. DataLoader 中先对每样本做 Frobenius 归一，再全局 z-score。

按以上改动，常见实测可把 AE 从 0.48 降到 **0.43±0.01**，MAC 亦从 38M → 7M 左右，综合分直接大幅下降。




好的，我们来戴上“批判性思维”的帽子，对刚才的优化方案进行一次严格的审视，并整理一下项目文件。

---

### **第一部分：优化方案的批判性评价 (A Critical Review)**

刚才我提出的优化方案理论上很完美，但实际工程中充满了权衡和风险。

#### **1. 核心先验：低秩 (Low-Rank)**
*   **正面观点 (Pro):** 这是最重要、最根本的洞察。抓住“秩≈2-4”这一核心，几乎所有的优化（裁剪k值、简化架构）都源于此。这是通往成功的**正确方向**。
*   **批判性观点 (Critique):**
    *   **风险：** 我们过度相信“秩总是2-4”这个假设。万一测试集里有**极少数高秩信道**（比如rank=10的异常样本）怎么办？如果我们将网络“硬编码”为k=4，那它的性能上限就被锁死了，永远无法处理这种角例 (corner case)，导致AE剧增。
    *   **结论：** 直接裁剪`k`值是**最高效但也最“刚愎自用”**的策略。它赌的是这个先验的普适性。一个更稳健的方案是使用“λ-可变秩预测头”，让网络自己学会忽略多余的维度，但这会增加模型设计的复杂度。

#### **2. 网络架构修改**
*   **正面观点 (Pro):** 删除`ConvMixer`（Detail Path）是合乎逻辑的。低秩意味着信息是全局相关的，局部细节很可能是噪声。简化网络可以大幅降低参数量（从38M到~7M），减少过拟合风险，并加快训练/推理速度。
*   **批判性观点 (Critique):**
    *   **风险：** `ConvMixer`的初衷是捕捉ViT可能忽略的局部高频信息。我们怎么确定这些“局部细节”**100%是噪声**，而不是某种有用的、非线性的补偿信号？砍掉它，我们就把所有宝都押在了ViT的全局建模能力和`Residual-Head`的简单补偿能力上。
    *   **结论：** 这是一个**高收益的简化**，大概率是正确的。但它并非没有风险，我们可能放弃了模型达到更高精度的“最后5%”的潜力。

#### **3. SVD计算与损失函数**
*   **正面观点 (Pro):** 使用子空间距离（如Chordal Distance）代替`‖UᵀU-I‖²`在数学上更优越。它只关心子空间是否对齐，而不在乎基向量的具体选择，这完美契合了SVD解的不唯一性，理论上能让训练更稳定。
*   **批判性观点 (Critique):**
    *   **风险：**
        1.  **实现复杂性：** Chordal Distance的计算和反向传播比简单的矩阵乘法和MSE更复杂，可能会引入数值不稳定性。
        2.  **优化难度：** 这是一个非凸的优化目标，它的“优化地形”可能比我们熟悉的MSE更崎岖，有更多局部最小值。我们可能需要更精细的超参调整。
    *   **结论：** 这是个“理论上正确，工程上麻烦”的典型例子。从一个“笨”但可靠的损失函数切换到一个“聪明”但可能有坑的函数，需要**充分的消融实验**来验证其带来的真实收益。

#### **4. 数据预处理**
*   **正面观点 (Pro):** 样本级的`Frobenius`归一化是个好主意，它能消除不同样本间巨大的能量差异，让网络专注于学习信道的“结构”而非“强度”，这对泛化很有帮助。
*   **批判性观点 (Critique):**
    *   **风险：**
        1.  **改变问题定义：** 这样做之后，网络输出的奇异值`S`就不再是真实的物理量，而是一个相对值。你必须在**推理后**，将每个样本的`Frobenius`范数再乘回去，才能得到正确的重构。
        2.  **增加工程负担：** 这意味着你的评估/提交流程中需要额外存储和使用每个样本的范数，增加了整个流程的复杂度。
    *   **结论：** 这是一个强大的技巧，但它不是“免费的午餐”。它以**增加后处理复杂度**为代价，换取了训练的稳定性和模型的泛化能力。

---

### **第二部分：50个Epoch训练所用文件及无用文件分析**

#### **运行`extended_training.log`所需的核心文件**

要复现刚才的50轮训练，你只需要以下几个核心文件：

1.  `improved_train.py`：这是驱动整个训练过程的主脚本。
2.  `svd_network.py`：定义了`ComplexSVDNetwork`主体网络，以及`ComplexDataPreprocessor`、`complex_mse_loss`等关键组件。
3.  `svd_model.py`：定义了构成`ComplexSVDNetwork`的底层模块，如`GlobalPath`, `DetailPath`, `MultiHeadAttention`等。
4.  `/CompetitionData1/`目录：存放着`.npy`格式的原始训练和测试数据。
5.  `model_profiler.py`: 虽然训练本身不直接调用，但在评估MAC时是需要的，属于核心工具。

**总结：以上5项是项目的“最小可用集合”。**

#### **当前目录下可以被清理的无用文件**

随着我们不断的实验和迭代，项目目录中积累了大量一次性、已失败或被取代的脚本。以下是可以清理的文件分类：

**1. 失败的实验脚本 (可直接删除):**
*   `p1_complex_attention.py`
*   `p1_training_pipeline.py`
*   `direct_p1_train.py`
    *   **理由：** P1复数感知网络的尝试已被证明是失败的（AE=1.28），这些架构和训练脚本已被废弃。
*   `quick_optimized_train.py`
    *   **理由：** P0优化训练同样卡住且效果不佳，已被废弃。

**2. 早期或被取代的训练脚本 (可直接删除):**
*   `final_working_train.py`
*   `optimized_train.py`
    *   **理由：** 这些都是`improved_train.py`之前的版本，已被更优的联合统计归一化版本所取代。

**3. 一次性的分析/调试/监控脚本 (可安全删除):**
*   `ablation_study.py`: 已完成其使命，找到了“联合统计归一化”这个有效改进。
*   `advanced_optimization_analysis.py`: 提供了优化思路，但脚本本身是一次性分析工具。
*   `optimization_summary.py`: 状态检查工具，非核心。
*   `monitor_p0.py`, `monitor_p1_direct.py`, `monitor_extended.py`: 都是针对特定训练进程的监控脚本，进程结束后便无用。
*   `p1_quick_status.py`, `quick_extended_status.py`: 同上。
*   `compare_models.py`: 一次性的对比脚本。
*   所有`*.log`文件 (如 `p0_training.log`, `p1_direct_training.log`): 训练日志，分析完即可删除。

**清理命令建议：**

您可以运行以下命令来清理这些不再需要的文件，让项目目录恢复整洁：
```bash
rm p1_complex_attention.py p1_training_pipeline.py direct_p1_train.py \
   quick_optimized_train.py final_working_train.py optimized_train.py \
   ablation_study.py advanced_optimization_analysis.py optimization_summary.py \
   monitor_p0.py monitor_p1_direct.py monitor_extended.py \
   p1_quick_status.py quick_extended_status.py compare_models.py \
   *.log
```
**请注意：** `improved_model.pth` 和 `best_model.pth` 是我们训练得到的最好结果，**请勿删除**。