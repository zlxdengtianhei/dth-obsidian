
# 方法

核心思想是在预训练的语言模型和预训练的视觉-语言模型之间建立一个无需训练的接口。理想情况下，生成的字幕应该全面覆盖图像中的信息，并与问题相关。我们通过使用基于**显著性图的可解释性技术**来识别与问题最相关的图像区域，并仅从这些区域生成字幕来促进相关性。此外，我们通过**注入随机性来促进覆盖度**，包括随机采样相关图像区域和文本标记过程中的字幕生成。

![[Pasted image 20231207164805.png]]
GradCAM：这是一种基于特征归因的可解释性技术，它聚合了所有跨注意力图并使用梯度的权重。

BLIP：这是一个大规模预训练的视觉-语言模型，它包含一个输出图像v和文本t之间相似度分数sim(v, t)的网络分支。

UnifiedQAv2


#### 1. 匹配图像区域和问题
因此，我们鼓励PNP-VQA生成**描述与问题相关的图像区域的字幕**，而不是没有特定目的的通用字幕。我们通过利用BLIP（Li等人，2022b），这是一个大规模预训练的视觉-语言模型，它包含一个输出图像v和文本t之间相似度分数sim(v, t)的网络分支。

$X ∈ R^{K×D_v}$，其中K是图像区域的数量，Dv是图像特征维度。我们将文本特征表示为 $Y ∈ R^{M×D_t}$，其中M是文本标记的数量，Dt是文本特征维度。对于每个跨注意力头，我们有参数矩阵 $W_Q ∈ R^{Dt×D_t}$ 和 $W_K ∈ R^{D_v×D_t}$。跨注意力分数 $A ∈ R^{M×K}$ 可以写成:

![[Pasted image 20231208182815.png]]


A的第j行指示第j个文本标记分配给所有图像块的关注量。在ITE网络的选定层，我们计算相似性得分的导数w.r.t交叉注意力得分，∂ sim(v, t)/∂A，并将梯度矩阵逐元素乘以交叉注意力得分。第i个图像补丁的相关性rel（i）取H个注意力头上的平均值和M个文本标记上的总和：

![[Pasted image 20231208183414.png]]

注意力头和图像块的区别是什么



#### 字幕的生成和筛选

为了生成字幕，系统将根据相关性采样的图像区域作为输入。每个区域的字幕生成是独立的，以确保生成的字幕尽可能覆盖图像的不同方面。生成过程中，系统通过添加一个简短的提示语“a picture of”来开始字幕的生成。此外，为了避免重复，系统会检查新生成的字幕是否与之前的字幕完全重复，只保留独特的字幕。
###### 字幕生成中的随机top-k采样
随机top-k采样是一种在生成字幕时选择下一个词的策略。在这种方法中，模型会在每一步生成多个候选词，然后从中随机选择一个。这种方法与束搜索不同，后者倾向于选择概率最高的词，可能导致生成的字幕单调且重复。通过随机top-k采样，模型可以探索更多的可能性，生成更丰富和多样的字幕。

#### 问题回答模块

问答模块的核心是一个预训练的编解码器，专门处理文本数据。由于此模块不能直接处理图像数据，因此需要将问题和生成的所有字幕作为文本输入。由于输入可能非常长，PNP-VQA采用了融合解码（FiD）策略。FiD与融合编码（FiE）不同，后者将问题和所有字幕连接成一个长段落作为编码器的输入。

###### 融合解码（FiD）策略
融合解码（Fusion-in-Decoder）策略是处理长输入的有效方法。在这种策略中，模型会单独对每个输入片段进行编码，然后将所有编码的片段传递给解码器进行处理。这与融合编码（Fusion-in-Encoder）策略不同，后者将所有输入连接成一个长段落，然后一起编码。FiD策略允许模型更有效地处理和整合来自多个源的信息，特别是在处理长输入时。


# 数据集和评估

我们采用了多个零样本视觉问答（VQA）基准测试，包括VQAv2（Goyal等人，2017）的验证集（214354个问题）和测试开发集（107394个问题），OK-VQA（Marino等人，2019）的测试集（5046个问题）以及GQA平衡版（Hudson和Manning，2019）的测试开发集（12578个问题）。我们包含了VQAv2的验证集，因为一些近期的研究（如Tsimpoukelli等人，2021；Jin等人，2022）仅在该数据集上评估他们的性能。我们通过开放式生成获取答案，并基于精确匹配进行评估。我们对VQAv2和OK-VQA报告软准确度（soft-accuracy，Goyal等人，2017），以考虑到多个正确答案的情况；对于GQA，我们报告标准准确度。
