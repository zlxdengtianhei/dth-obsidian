各位评委老师大家好，我是212821235张乐轩，我的毕设题目为零样本视觉问答模型的设计与实现


我今天将从这四个部分完成我毕设项目的介绍


首先是选题背景及意义
视觉问答任务是基于图像理解回答自然语言问题的多模态任务，现有的方法很多依赖于大规模的标注数据，而最新的轻量化的预训练全景分割编码器，通过多模态信息融合，可以实现更细粒度视觉理解的零样本推理

通过这样的方式可以减少对标注数据的依赖，同时提升预训练模型在陌生场景的能力，为多个行业提供技术支撑

下面我将介绍多模态语义增强的零样本视觉问答模型的创新点与整体架构。
创新点1是BlipEsamITM多模态融合与分割模块，模块架构主要由三个部分组成，视觉编码器，文本编码器以及sam掩码解码器。
此模块结合了BLIP的图文匹配能力，以及EfficientSAM的高效分割功能，让模型能够实现对文中关键对象的分割提取。

具体的实现过程如流程图所示，其中的关键在于利用BLIP生成的热力图，采用自适应显著点提取算法，选出热力图中表示出的最相关点，通过坐标变换，作为SAM掩码解码器的空间位置提示，从而实现文中提到的实例对象的掩码分割。


下面我将介绍DifnetEncoder特征融合模块架构


整体采用渐进式五阶段融合策略，通过超参数Lf (融合层位置)、T (每层重复次数)、N (总层数) 精细控制处理流程。各阶段核心操作如下：  

接收来自BlipITM视觉编码器 (F₁) 及BlipESAMITM增强视觉编码器 (F₂) 的两组视觉特征。并为两组特征创建注意力掩码 (M₁、M₂)

阶段2 (独立特征增强)：在前Lf层，两个特征流 并行且独立地进行处理。每层通过包含多头自注意力 (MHSA) 和位置前馈网络 (PWFF) 的EncoderLayer进行T次重复处理，旨在强化各流的特征表示，充分捕获各自特征空间内的图像块间上下文关系。  

阶段3 进行特征融合：在第Lf层执行核心的融合操作。将经过独立增强的两个特征流Z₁(Lf) 和 Z₂(Lf) 进行逐元素加法融合。融合后的特征 (Z₁(Lf) + Z₂(Lf)) 立即通过一个EncoderLayer（使用M₁作为掩码），促进新融合表示的自交互与信息整合。  

之后进行融合后深度优化：从Lf+1层至N-1层，对已融合的特征Z(l) (其中 l > Lf) 进行持续的深度处理。每一层继续应用EncoderLayer处理，旨在进一步挖掘融合特征空间中的复杂关系，提升表示的统一性和丰富性。  

最终处理后的特征ZN与融合前的两个独立流特征Z₁(Lf) 和 Z₂(Lf) 进行残差相加。这种设计保留了早期阶段的有效信息，防止信息丢失，也实现了两个。

- 独立增强：各流通过MHSA与PWFF独立强化，捕获特有上下文。
- 加性融合：在Lf层逐元素相加，高效整合双流信息并初步交互。
- 深度优化：融合后特征继续经多层MHSA与PWFF处理，深化理解。
- 残差连接：整合深层输出与融合前信息，保障信息流与梯度。

多流特征融合作用

DifnetEncoder继承自MultiLevelEncoder，智能融合BlipITM标准视觉编码器和BlipEsamITM增强视觉编码器的双流特征，同时利用一般视觉语义信息和细粒度对象/区域信息。

融合特征生成热力图突显重要图像区域，融合全局匹配特征和局部分割特征，提供更全面的视觉理解能力。

接下来是多模态语义增强的零样本视觉问答模型架构
整体模型架构如图所示，
整体架构采用模块化的设计，由图像 问题匹配模块，图像描述生成模块，问答模块三个核心子模块组成，具有即插即用的特性。
模型核心技术在于多模块协同。图文匹配利用基于BLIP架构的BlipITM模块及集成EfficientSAM提升定位的BlipESamITM模块。其输出通过DifnetEncoder特征融合整合视觉信息，并结合Grad-CAM技术生成热力图，突显问题相关区域。随后，BLIPCaption模型利用热力图引导的采样（辅以top-k/top-p采样控制多样性）生成图像描述。最终，PNPUnifiedQAv2FiD问答模型基于T5架构和FiD机制，并行处理多问题-描述对，融合上下文信息生成答案。


与通过端到端视觉-语言训练联合的预训练模型相比，PNP-VQAbase在**VQAv2数据集的测试开发集**上达到了**55.2%**的准确率，这一成绩超过了包括FewVLMlarge（47.7%）和VLKDViT-L/14（44.5%）在内的多个模型。在更侧重细粒度场景理解和组合推理的**GQA数据集的测试开发集**上，PNP-VQAbase的准确率达到**34.6%**，显著优于FewVLMlarge的29.3%。

这一性能优势体现了PNP-VQAbase所采用的“**通过自然语言与零样本训练联合**”策略的有效性。该策略允许模型充分利用预训练语言模型**UnifiedQAv2**的强大问答能力和世界知识，同时通过BlipITM模块捕获图像的视觉特征。值得注意的是，PNP-VQAbase在参数量上相对较小，却能在某些指标上达到甚至优于参数量巨大的模型（例如Flamingo系列的部分版本在VQAv2上的表现为49.2%-51.8%），这突显了其架构设计的高效性。