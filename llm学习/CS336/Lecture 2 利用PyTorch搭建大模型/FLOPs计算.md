`x --w1--> h1 --w2--> h2 -> loss`

- **输入 `x`**: 形状为 `(B, D)`，其中 `B` 是批量大小（Batch Size），`D` 是输入特征维度。
- **权重 `w1`**: 形状为 `(D, D)`，是第一个线性层的权重矩阵。
- **中间变量 `h1`**: 形状为 `(B, D)`，是第一个线性层的输出。
- **权重 `w2`**: 形状为 `(D, K)`，是第二个线性层的权重矩阵。
- **输出 `h2`**: 形状为 `(B, K)`，是第二个线性层的输出，`K` 是输出类别或维度。
- **损失 `loss`**: 一个标量（scalar），由 `h2` 计算而来。

**前向传播计算:**
![[Pasted image 20251010233756.png]]

1. `h1 = x @ w1`
    
    - 这是一个矩阵乘法：`(B, D) @ (D, D)`，结果的形状是 `(B, D)`。
    - 为了计算 `h1` 中的任意一个元素 `h1[i][k]`，我们需要计算 `sum_j(x[i][j] * w1[j][k])`。
    - 这个计算涉及 `D` 次乘法和 `D-1` 次加法，约等于 `2 * D` 次浮点运算（FLOPs）。
    - `h1` 矩阵总共有 `B * D` 个元素。
    - 所以，总的 FLOPs 约等于 `(B * D) * (2 * D) = 2 * B * D * D`。
2. `h2 = h1 @ w2`
    
    - 这是一个矩阵乘法：`(B, D) @ (D, K)`，结果的形状是 `(B, K)`。
    - 为了计算 `h2` 中的任意一个元素 `h2[i][k]`，我们需要计算 `sum_j(h1[i][j] * w2[j][k])`。
    - 这个计算涉及 `D` 次乘法和 `D-1` 次加法，约等于 `2 * D` 次 FLOPs。
    - `h2` 矩阵总共有 `B * K` 个元素。
    - 所以，总的 FLOPs 约等于 `(B * K) * (2 * D) = 2 * B * D * K`。
3. `loss = h2.pow(2).mean()`
    
    - 这包括对 `h2` 的每个元素进行平方（`B*K` 次乘法）和求平均（`B*K-1` 次加法和1次除法）。与矩阵乘法相比，这部分的计算量很小，通常在估算FLOPs时可以忽略不计。

**前向传播总FLOPs:**  
将主要计算（两个矩阵乘法）的FLOPs相加，得到：  
`num_forward_flops = (2 * B * D * D) + (2 * B * D * K)`  
这与代码中的注释完全一致。

---

### 2. 反向传播和链式法则

反向传播的目的是计算损失函数 `loss` 相对于模型参数（`w1` 和 `w2`）的梯度（`d(loss)/d(w1)` 和 `d(loss)/d(w2)`）。这个过程依赖于微积分中的**链式法则**。

链式法则告诉我们，如果 `loss` 是 `h2` 的函数，`h2` 是 `h1` 和 `w2` 的函数，而 `h1` 又是 `x` 和 `w1` 的函数，那么我们可以像链条一样，从后向前计算梯度。

- `d(loss)/d(w2) = d(loss)/d(h2) * d(h2)/d(w2)`
- `d(loss)/d(w1) = d(loss)/d(h1) * d(h1)/d(w1) = (d(loss)/d(h2) * d(h2)/d(h1)) * d(h1)/d(w1)`

在 PyTorch 中，`loss.backward()` 会自动完成这个过程。它会计算 `loss` 相对于计算图中所有 `requires_grad=True` 的张量的梯度。我们这里要做的，就是手动拆解这个过程，并计算其FLOPs。

我们将从计算图的末端开始，一步步向后传播梯度。

---

### 3. 逐层计算反向传播的FLOPs
![[Pasted image 20251010233839.png]]

在 PyTorch 中，`tensor.grad` 存储的是 `d(loss)/d(tensor)`。

**第0步：损失函数的梯度**

- `h2.grad = d(loss)/d(h2)`
- 因为 `loss = h2.pow(2).mean() = (1/(B*K)) * sum(h2[i,k]^2)`，所以 `d(loss)/d(h2[i,k]) = (2/(B*K)) * h2[i,k]`。
- 这是一个逐元素的操作，计算量很小，我们同样忽略不计。`h2.grad` 的形状和 `h2` 一样，是 `(B, K)`。

**第1步：反向传播通过 `h2 = h1 @ w2`**

这一步的目标是，利用已知的 `h2.grad` (`d(loss)/d(h2)`)，计算出 `w2.grad` (`d(loss)/d(w2)`) 和 `h1.grad` (`d(loss)/d(h1)`)。

- **计算 `w2.grad` (`d(loss)/d(w2)`)**
    
    - **链式法则（元素级）**: `d(loss)/d(w2[j,k]) = sum_i (d(loss)/d(h2[i,k]) * d(h2[i,k])/d(w2[j,k]))`
    - 因为 `h2[i,k] = sum_m(h1[i,m] * w2[m,k])`，所以 `d(h2[i,k])/d(w2[j,k])` 当 `m=j` 时等于 `h1[i,j]`，否则为0。
    - 代入后得到：`d(loss)/d(w2[j,k]) = sum_i (h2.grad[i,k] * h1[i,j])`。
    - 这正是代码注释 `w2.grad[j,k] = sum_i h1[i,j] * h2.grad[i,k]` 的含义。
    - **矩阵形式**: 这个求和操作等价于一个矩阵乘法：`w2.grad = h1.T @ h2.grad`。
    - **FLOPs 计算**:
        - `h1.T` 的形状是 `(D, B)`。
        - `h2.grad` 的形状是 `(B, K)`。
        - 执行 `(D, B) @ (B, K)` 得到 `(D, K)` 形状的 `w2.grad`。
        - 计算量为 `D * K * (2 * B) = 2 * B * D * K`。
        - 所以代码中 `num_backward_flops += 2 * B * D * K` 是正确的。
- **计算 `h1.grad` (`d(loss)/d(h1)`)**
    
    - **链式法则（元素级）**: `d(loss)/d(h1[i,j]) = sum_k (d(loss)/d(h2[i,k]) * d(h2[i,k])/d(h1[i,j]))`
    - 因为 `h2[i,k] = sum_m(h1[i,m] * w2[m,k])`，所以 `d(h2[i,k])/d(h1[i,j])` 当 `m=j` 时等于 `w2[j,k]`，否则为0。
    - 代入后得到：`d(loss)/d(h1[i,j]) = sum_k (h2.grad[i,k] * w2[j,k])`。
    - 这正是代码注释 `h1.grad[i,j] = sum_k w2[j,k] * h2.grad[i,k]` 的含义。
    - **矩阵形式**: 这个操作等价于矩阵乘法：`h1.grad = h2.grad @ w2.T`。
    - **FLOPs 计算**:
        - `h2.grad` 的形状是 `(B, K)`。
        - `w2.T` 的形状是 `(K, D)`。
        - 执行 `(B, K) @ (K, D)` 得到 `(B, D)` 形状的 `h1.grad`。
        - 计算量为 `B * D * (2 * K) = 2 * B * D * K`。
        - 所以代码中 `num_backward_flops += 2 * B * D * K` 也是正确的。

**第2步：反向传播通过 `h1 = x @ w1`**

现在我们有了 `h1.grad` (`d(loss)/d(h1)`)，可以继续向后传播，计算 `w1.grad` (`d(loss)/d(w1)`)。

- **计算 `w1.grad` (`d(loss)/d(w1)`)**
    
    - 这一步的计算结构与上一步计算 `w2.grad` 完全相同。只是角色发生了变化：
        - `h2` 对应 `h1`
        - `h1` 对应 `x`
        - `w2` 对应 `w1`
        - `h2.grad` 对应 `h1.grad`
    - 所以，我们可以直接套用之前的矩阵公式：`w1.grad = x.T @ h1.grad`。
    - **FLOPs 计算**:
        - `x.T` 的形状是 `(D, B)`。
        - `h1.grad` 的形状是 `(B, D)`。
        - 执行 `(D, B) @ (B, D)` 得到 `(D, D)` 形状的 `w1.grad`。
        - 计算量为 `D * D * (2 * B) = 2 * B * D * D`。
- **计算 `x.grad` (`d(loss)/d(x)`)** (虽然代码中说不需要，但为了完整性我们分析一下)
    
    - 同理，这对应于上一步计算 `h1.grad`。
    - 套用公式：`x.grad = h1.grad @ w1.T`。
    - **FLOPs 计算**:
        - `h1.grad` 的形状是 `(B, D)`。
        - `w1.T` 的形状是 `(D, D)`。
        - 执行 `(B, D) @ (D, D)` 得到 `(B, D)` 形状的 `x.grad`。
        - 计算量为 `B * D * (2 * D) = 2 * B * D * D`。

**反向传播总FLOPs:**  
代码的最后一行 `num_backward_flops += (2 + 2) * B * D * D` 正是把计算 `w1.grad` 和 `x.grad` 的FLOPs加在了一起：  
`(2 * B * D * D) (for w1.grad) + (2 * B * D * D) (for x.grad) = (2+2) * B * D * D`

所以，总的反向传播FLOPs为：  
`num_backward_flops = (2*B*D*K) (for w2.grad) + (2*B*D*K) (for h1.grad) + (2*B*D*D) (for w1.grad) + (2*B*D*D) (for x.grad)`  
（注意：实际计算参数梯度时，我们只需要 `w1.grad` 和 `w2.grad`，但为了将梯度传播下去，中间变量的梯度如 `h1.grad` 也必须计算。而 `x.grad` 只有在需要更新输入或网络更深时才需要。）

---

### 4. 总结与通用性分析

**计算量对比**

- **前向传播**: `Forward = (2 * B * D * D) + (2 * B * D * K)`
- **反向传播**: `Backward = (2*B*D*D for w1.grad) + (2*B*D*D for x.grad) + (2*B*D*K for w2.grad) + (2*B*D*K for h1.grad)`  
    `Backward = 2 * [(2 * B * D * D) + (2 * B * D * K)] = 2 * Forward`

我们得出一个非常重要的结论：**对于一个由线性层（或卷积层）主导的网络，反向传播的计算量大约是前向传播的两倍。**

**为什么这个公式是通用的？**

这个结论之所以具有通用性，是因为它揭示了矩阵乘法这一基本运算在前向和反向传播中的对偶关系。

考虑一个通用的线性层：`Y = X @ W`

1. **前向传播**:
    
    - 执行 **1** 次矩阵乘法：`X @ W`。
2. **反向传播**:
    
    - 假设我们已经从更深的层获得了 `Y.grad` (`d(loss)/dY`)。
    - 我们需要计算 `W.grad` 和 `X.grad`。
    - **计算 `W.grad`**: `W.grad = X.T @ Y.grad`。这是 **第1次** 矩阵乘法。
    - **计算 `X.grad`**: `X.grad = Y.grad @ W.T`。这是 **第2次** 矩阵乘法。

**核心规律**: 对于任何一个 `Y = X @ W` 形式的线性层，它的前向传播包含 **1** 次矩阵乘法，而它的反向传播（计算其输入的梯度和权重的梯度）需要 **2** 次矩阵乘法。

**推广到更深的网络:**  
如果一个网络由许多线性层堆叠而成 `L1, L2, ..., LN`，那么：

- **前向传播** 会依次执行 `L1, L2, ..., LN` 的前向计算。
- **反向传播** 会以相反的顺序 `LN, ..., L2, L1` 执行反向计算。每一层的反向计算都遵循上述 "1次变2次" 的规律。

因此，整个网络的反向传播计算量大约是前向传播的两倍。这个 `~2x` 的比例对于深度学习模型的性能和硬件设计（例如，NVIDIA 的 Tensor Cores 设计就同时考虑了前向和反向传播的计算模式）具有重要的指导意义。

**不同位置的计算:**

- **网络末端的层 (如 `w2`)**: 它的反向传播接收来自损失函数的直接梯度。
- **网络中间的层 (如 `w1`)**: 它的反向传播接收来自后一层（`w2`所在层）传播回来的梯度（即 `h1.grad`）。

无论层在哪个位置，其反向传播的计算模式都是一样的：利用“输出梯度”，通过两次矩阵乘法，分别计算出“权重梯度”和“输入梯度”。这就是为什么这个计算公式是通用的。