![[Pasted image 20251011202947.png]] 
在transformer块中，因为需要进行残差连接，需要输入和输出同样大小，否则需要做投影。因此块的输入输出使用同一个维度

![[Pasted image 20251011205239.png]]
### Layer Normalization (LN) 在语言数据中的优势

在处理语言序列数据时，Layer Normalization (LN) 通常是比 Batch Normalization (BN) 更好的选择，其核心优势可以总结如下：

Layer Normalization 的归一化是**“横向”的，针对单个样本内部进行**。对于一个输入样本（例如一个句子），LN 会收集这个样本在**所有序列位置（sequence steps）**和**所有特征维度（feature dimensions）**上的全部数据，计算出一个属于该样本自身的均值和方差。然后，它使用这个均值和方差来归一化这个样本自己。

这种方式带来了三大优势：

1. **独立于批次大小（Batch Size）**：LN 的计算完全在单个样本内部完成，不依赖批次中的任何其他样本。这使得它在批次大小很小（例如1或2）的情况下依然能稳定工作，这对于训练消耗显存巨大的大型语言模型至关重要。
2. **完美适应可变序列长度**：由于归一化是针对单个样本的全部特征进行的，所以无论这个样本的序列长度是10还是1000，计算方式都保持一致。这自然地解决了NLP任务中句子长度不一的问题。
3. **训练与推理行为一致**：LN 在训练和推理时执行完全相同的计算逻辑，因为它不需要像BN那样预先计算或依赖全局统计数据。这使得模型部署更简单，行为更可预测。

综上所述，LN通过在样本层面对特征进行动态的、整体的规范化，有效稳定了深层Transformer等模型的内部激活值分布，极大地提升了训练的稳定性和最终性能，使其成为现代NLP模型的标准配置。

---

### Batch Normalization (BN) 在其适用场景的解释

与LN相对，Batch Normalization 的归一化是**“纵向”的，跨越整个批次进行**。它针对**每一个独立的特征维度**（例如，图像的一个通道），收集批次中所有样本在该特征维度上的值，计算出一个属于该特征的公共均值和方差。然后，它使用这个均值和方差来归一化所有样本的这个特征。

BN 在计算机视觉（CV）领域的卷积神经网络（CNN）中表现出色，主要原因如下：

1. **利用批次信息进行正则化**：BN 的计算引入了由当前批次数据带来的轻微噪声，这种噪声可以起到一定的正则化效果，有助于提升模型的泛化能力。
2. **特征具有一致的语义**：在图像任务中，一个特征图（Feature Map）的同一个通道在不同样本的相同空间位置上，通常具有相似的语义（例如，都表示某种纹理或边缘）。因此，将它们放在一起进行归一化是合理且有效的。
3. **对大批次大小的依赖**：BN 的效果高度依赖于一个足够大的批次大小，这样计算出的批次统计量才能稳定地近似全局的统计量。在典型的图像任务中，使用较大的批次是常见做法，这满足了BN的工作前提。

然而，BN的这些特性也使其不适用于语言序列：它难以处理可变序列长度，并且在推理时需要使用训练阶段累积的全局统计量，这使得它无法泛化到比训练数据更长的序列。

---

### 归一化层的可学习参数与计算公式

无论是 LayerNorm 还是 Batch Norm，它们都不仅仅是简单的归一化。为了保持网络的表达能力，它们都引入了两个可学习的参数，并在归一化后进行一次仿射变换。

#### 可学习参数及其作用

1. **`γ` (gamma - 缩放因子)**: 这是一个可学习的参数，其维度与被归一化的特征维度相匹配。它被初始化为1。
    
    - **作用**：`γ` 负责对归一化后的数据进行**重新缩放**。它允许网络自主学习每个特征维度的最佳“尺度”或“重要性”。如果网络认为某个特征的方差应该更大，它可以通过学习一个大于1的`γ`值来实现。
2. **`β` (beta - 平移因子)**: 这是一个可学习的参数，维度与`γ`相同。它被初始化为0。
    
    - **作用**：`β` 负责对归一化后的数据进行**重新平移**。它允许网络自主学习每个特征维度的最佳“偏移”或“均值”。如果网络认为某个特征的均值不应该是0，它可以通过学习一个非零的`β`值来调整。

**核心思想**：`γ` 和 `β` 赋予了网络“撤销”或“调整”归一化操作的能力。在最极端的情况下，如果网络发现归一化对当前层是有害的，它可以学习 `γ = σ`（标准差）和 `β = μ`（均值），从而近乎完美地还原出原始的输入。这使得归一化层在提供稳定性的同时，不会破坏网络学到的宝贵信息。

#### 统一的计算公式

两个归一化层的核心计算流程可以统一用以下公式表示。它们的**唯一区别**在于计算均值 `μ` 和方差 `σ²` 时，**对输入张量 `x` 取平均值的维度集合不同**。

**计算公式：**

`y = γ * ( (x - μ) / sqrt(σ² + ε) ) + β`

**公式中每个参数的介绍：**

- `x`: 输入张量。这是上一层网络的输出。
- `μ`: 均值（Mean）。对输入张量 `x` 的特定维度进行求平均计算得到。
    - 在 **Batch Norm** 中，`μ` 是在**批次维度 `N`** 和**空间维度（如 `H`, `W`）**上计算的，但对**通道/特征维度 `C`** 是独立的。
    - 在 **Layer Norm** 中，`μ` 是在**通道/特征维度 `C`** 和**空间维度（如 `H`, `W`）**上计算的，但对**批次维度 `N`** 是独立的。
- `σ²`: 方差（Variance）。计算方式与 `μ` 类似，只是计算的是方差。它衡量了数据在相应维度上的离散程度。
- `ε` (epsilon): 一个非常小的正数（例如 `1e-5`）。
    - **作用**：它被加到方差上，以防止分母（标准差）为零，从而保证数值计算的稳定性。
- `(x - μ) / sqrt(σ² + ε)`: 这是核心的**归一化步骤**。它将输入数据 `x` 的分布调整为均值接近0，方差接近1的标准分布。
- `γ`: **可学习的缩放参数**。如上所述，它用于调整归一化后数据的尺度。
- `β`: **可学习的平移参数**。如上所述，它用于调整归一化后数据的偏移。
- `y`: 输出张量。这是经过归一化和仿射变换后的最终结果，它将被传递给下一层网络。


## 缩放点积注意力机制
n为n各query，每个query的长度为d_k, 必须与k的相同，K的数量可以为m与n不同，V的数量必须与K的数量m相同，长度与d_k相同。
两个向量点积的值越接近1，越相似。
为什么进行缩放就是在如果d_k相对比较大，那导致各个数值可能差异较大，这样经过softmax后最大的值可能很接近1，这样的话最终梯度可能很小，导致模型训练不动。
![[Pasted image 20251011214240.png]]
![[Pasted image 20251011214422.png]]
在mask中，因为在解码器中，在预测q_t时，不应该看到t之后的内容，所以对t之后的数加一个mask，将它变为一个非常大的负数，经过softmax后，就会变为0

