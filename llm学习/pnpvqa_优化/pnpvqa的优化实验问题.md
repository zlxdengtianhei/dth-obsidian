首先我想知道，zvqaf这个文章具体是做了什么
其次，我想知道，是否还有零样本vqa在2024，2025年的新发展？
另外，mobilesam，以及yolo11n-seg,具体是使用什么作为输入？是不是图像输入就够了，还是需要用户的点击信息？
另外告诉我envqa相关的信息，以及**Region-based VQA**的相关信息

vlpm模型，MoR，是什么，
Fastsam，mobilesamv2, yolo11seg
1. **对象关系感知VQA**：
    
    - 不仅关注单个对象，还建模对象间的空间和语义关系
    - 可能对需要理解场景结构的复杂问题特别有效
2. **自适应视觉信息选择**：
    
    - 根据问题类型动态选择最相关的视觉信息(图像块或对象)
    - 可能提高模型对不同类型问题的适应性
3. **多粒度视觉-语言对齐**：
    
    - 在像素、对象和场景多个粒度上对齐视觉和语言表示
    - 可能提高模型对复杂视觉概念的理解能力
4. **优先考虑MobileSAMv2**：
    
    - 比FastSAM更快(10ms vs 40ms)
    - 与原SAM的掩码对齐度更高(0.73 vs 0.41)
    - 提供对象感知提示采样，无需用户提示
5. **采用混合视觉信息方案**：
    
    - 结合图像分块与对象分割，获取互补视觉信息
    - 设计有效的融合策略，平衡两种信息来源
    - 考虑使用FiD技术提高融合效果
6. **关注轻量化端到端实现**：
    
    - 探索视觉-文本交互式令牌剪枝等技术
    - 考虑使用BiomedCLIP与LLaMA-3等轻量级模型
    - 优化批处理和推理效率

img2llm

如何加入空间理解，使用svd，软关联来自高度相关特征的分散线索（IKA）
