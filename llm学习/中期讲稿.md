In stage of our training, we conducted experiments on several  Qwen 2.5 model combinations: 0.5B and 1.5B, 7B and 1.5B, and 7B and 3B. We use large model as the bottom model, small model as the top model.

At the beginning, due to computational limitations, we started with the **Qwen 2.5 0.5B and 1.5B combination**. During all training experiments, **we froze the parameters of both Qwen models** and **trained only the stitch layer**, in order to ensure that performance changes reflected the effectiveness of the stitching strategy itself rather than additional fine-tuning of the base models. The **stitch layer** at this stage was composed of a linear layer with a **ReLU** activation function. However, results were consistently unsatisfactory. To improve performance, we experimented with various approaches, including adjustments to initialization strategies, hyper parameters such as learning rate, gradient accumulation, learning rate warmup, and learning rate schedulers—which brought some improvement, though not significant. The result is illustrated by the **yellow bars** in the figure.

After analysis, we suspected that ReLU may have caused **information loss by zeroing out negative values**. Therefore, we modified the stitch layer to use only linear layers without activation functions. . We also tried switching the order of the two models—using **0.5B as the bottom model** and **1.5B as the top**. The corresponding results, shown by the **green bars** in the chart, demonstrated **modest performance gains**, though the stitched model still **underperformed compared to a standalone Qwen 2.5 0.5B model**.

Later, we decided to rent GPUs to test larger model combinations. In experiments with **Qwen 7B–1.5B** and **Qwen 7B–3B**, we observed clear performance gains in **index 20** , which is represented by the **blue bars** in the figure.

When we again reversed the order of the models (using 1.5B as the bottom and 7B as the top), performance dropped significantly—aligning with observations reported in the stitchllm paper. We infer that in smaller model settings (e.g., 0.5B–1.5B), the smaller number of parameters and lower accuracy may occasionally lead to better results when the order is inverted, but this phenomenon disappears with larger models.

After the test in index 20 using qwen 7b stitch to 1.5b, we have trained the combinations of  0.5B and 1.5B, 7B and 1.5B, and 7B and 3B to stitch them every 5 indices. The evaluation result will be shown in next section.

---

In terms of data, as we all know **high-quality training data** is crucial to building strong models. We used the **C4 dataset** for training. Because of the dataset’s enormous size, it cannot be fully loaded into memory, so we adopted a **streaming data loading** approach.  
Specifically, the **data processor** first sorts all C4 dataset **arrow files** to ensure that no file will be read twice. A **generator** then extracts each entry sequentially and writes it into a data entry  **buffer**. Once the buffer is full, the data is tokenized and added to a token buffer. Every **1024 tokens** in buffer are treated as one training sample and passed to the training function.

---

After lots of hyperparameter tuning, we settled on the following configuration:

- **Batch size**: 4 (with gradient accumulation when memory is limited)
- **Learning rate**: 2e-4
- **Warmup steps**: 200
- **Learning rate scheduler**: linear

Under this setup, the **training loss curve remained stable**, with no significant oscillation, and **loss decreased steadily** as shown in the image. Around **3000 training steps**, the model reached a satisfactory performance level. Continuing training to **15000 steps** yielded further improvement, but at a much slower rate and higher time cost. we selected **3,000 training steps** as the **optimal training length**, balancing both efficiency and performance.
