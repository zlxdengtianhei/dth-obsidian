### 摘要翻译（中文）

当前的图像描述（IC）方法基于输入视觉信息和部分生成的句子信息，依次预测文本词汇。但在大多数情况下，由于视觉信息的不足，部分生成的句子可能会主导目标词的预测，使得生成的描述与给定图像的内容无关。本文提出了一个双信息流网络（DIFNet），解决这个问题。DIFNet采用分割特征作为另一个视觉信息源，以增强视觉信息在预测中的作用。为了最大化两种信息流的利用，我们还提出了一个有效的特征融合模块，称为迭代独立层归一化（IILN），它可以通过公共LN层压缩最相关的输入，同时通过私有LN层保留每个流中的模态特定信息。实验表明，我们的方法能够增强预测对视觉信息的依赖，使词汇预测更专注于视觉内容，并因此在MSCOCO数据集上实现了新的最佳性能，例如在COCO Karpathy测试切分上达到136.2的CIDEr分数。

### 摘要详细解读

文章提出的DIFNet旨在解决图像描述中的一个关键问题：当前方法中文本生成过于依赖于部分生成的句子，而非图像本身的视觉信息。这种现象导致生成的描述有时与图像内容不相关。

#### 核心创新点：
1. **双信息流网络（DIFNet）**：DIFNet利用分割特征作为额外的视觉信息源，从而强化了视觉信息在预测过程中的作用。这种方法是对传统图像描述方法的重要补充，它通过加入更丰富的视觉信息来改进文本生成的准确性。

2. **迭代独立层归一化（IILN）特征融合模块**：这个模块的设计旨在有效融合来自不同源的特征（如分割特征和传统视觉特征）。IILN通过公共层归一化压缩最相关的输入，同时保留每个流中的特定信息，解决了在融合不同模态数据时常见的信息丢失问题。

#### 主要贡献：
1. **视觉依赖性增强**：DIFNet显著增强了预测对视觉信息的依赖，提高了描述的视觉相关性。这意味着模型在生成文本时更多地考虑到了图像内容，而非仅依赖于先前生成的文本。

2. **性能提升**：在MSCOCO数据集上，DIFNet实现了新的最佳性能，尤其是在CIDEr评分上达到了显著提高。这表明DIFNet在图像描述任务中具有优越的效果。

3. **模型通用性**：DIFNet的设计允许它与现有的多种图像描述模型和方法兼容，显示了其在不同框架和设置下的应用潜力。

#### 实验结果：
通过在MSCOCO数据集上的实验，DIFNet展示了其在图像描述任务中的有效性。特别是，在考虑到视觉内容的情况下，其生成的文本描述更加准确和相关。

综上所述，DIFNet代表了图像描述领域的一个重要进步，尤其是在增强模强化模型对图像内容的理解和描述能力。这种方法不仅提高了生成文本的准确性和相关性，而且还为未来的图像描述任务提供了新的视角和技术路径。

#### 研究意义和未来应用：
1. **深入理解图像内容**：DIFNet通过整合更丰富的视觉信息，帮助模型更深入地理解图像内容，这对于自动图像描述生成具有重要意义。

2. **提升人工智能的视觉感知能力**：该技术的应用不仅限于图像描述，还可能推广到其他需要视觉感知的领域，如自动驾驶、医疗影像分析等。

3. **创新的模态融合策略**：DIFNet中的IILN模块提供了一种新的融合不同类型数据的方法，这对于处理多模态数据（如图像、文本和声音）具有潜在的应用价值。

综合来看，DIFNet不仅在图像描述领域展现了卓越的性能，也为多模态数据处理和人工智能的视觉理解领域提供了新的思路和技术支持。



### 简介部分翻译（中文）

图像描述（IC）是一个根据给定图像生成自然语言描述的任务。这要求模型从多个角度理解给定的图像，包括识别对象、动作以及它们之间的关系，并为该图像生成语言描述。受到神经机器翻译的启发，编码器-解码器框架已被广泛应用于图像描述任务。编码器接受一组由离线CNN网络提取的视觉特征作为输入，并进一步将它们编码到视觉-语言空间。然后，解码器利用编码器提供的视觉信息和部分生成的字幕来预测下一个词。然而，现有方法的一个主要缺点是，来自视觉特征提取器的视觉信息不足，有时不准确。此外，高级视觉线索如概念的引入需要额外的融合模块来与视觉特征对齐。与此相反，本文考虑了一种新类型的线索，即分割图，其区域语义与网格特征自然对齐。在我们的方法中，我们提出了一个双信息流网络（DIFNet），它采用分割特征作为另一个视觉信息来源，以增强视觉信息对预测的贡献。

### 简介部分详细解读

#### 图像描述任务的挑战
1. **多方面的理解需求**：图像描述不仅需要识别图像中的对象和动作，还需要理解这些元素之间的关系，这对模型的视觉和语言处理能力提出了挑战。

2. **编码器-解码器框架的应用**：受神经机器翻译的影响，图像描述任务广泛采用了编码器-解码器框架。编码器处理视觉信息，而解码器基于这些信息生成语言描述。

#### 现有方法的局限性
1. **视觉信息的不足**：现有的图像描述方法通常受限于视觉特征提取器提供的信息不足，这可能导致生成的描述与图像内容不够贴合。

2. **高级视觉线索的融合问题**：尽管引入了如概念等高级视觉线索来补充视觉信息，但这些线索与传统的视觉特征之间存在语义不一致和空间错位问题，需要额外的复杂融合模块。

#### DIFNet的创新之处
1. **双信息流网络**：DIFNet通过将分割特征作为补充视觉信息的来源，强化了视觉信息在预测过程中的作用。这一新颖方法能更好地将图像的视觉内容与生成的语言描述对齐。

2. **分割图的利用**：DIFNet使用分割图作为一种新型的视觉线索。分割图直接将区域语义与网格特征对齐，提供了一种自然和有效的方式来整合视觉信息，这在以往的方法中是不常见的。

#### 研究意义
DIFNet的提出，不仅针对图像描述任务中的关键挑战提供了创新的解决方案，而且为深度学习领域中的视觉与语言融合提供了新的研究方向。通过更有效地利用视觉信息，DIFNet能够提高模型对图像内容的理解和描述能力。这种增强的视觉-语言对齐使得生成的描述不仅更加准确，而且更加丰富和细致。

DIFNet的这种方法开辟了图像描述领域的新途径，为未来深度学习中的视觉和语言融合研究提供了宝贵的启示。它展示了如何通过创新地结合不同类型的视觉信息源（如分割图），以及如何有效地在深度学习模型中融合这些信息，从而提高整体的描述质量和相关性。这为图像描述以及更广泛的视觉语言融合任务提供了重要的研究方向和实践方法。



### 相关工作部分翻译（中文）

#### 图像描述
编码器-解码器框架被广泛采用于图像描述模型。然而，大多数先前的方法遵循单一信息流管道，并通过增加模型复杂性来设计架构。最近的研究引入了概念、属性和标签来增强视觉语义，但很难与视觉特征对齐。与使用概念、属性和标签不同，本研究使用分割特征作为第二信息流来增强视觉表现。

#### 泛观分割
泛观分割任务统一了实例分割和语义分割任务。它可以识别像素的语义类别，同时为像“人”这样的类别提供实例边界。为了利用分割线索，HIP构建了一个层次解析架构，将实例级、区域级和图像级特征关联起来用于图像描述。与HIP不同，本研究使用分割图来构建结构化视觉语义表示，保留原始图像的空间结构信息，更容易与网格特征融合。

#### 多模态融合
许多研究致力于多模态融合。早期方法使用简单的聚合操作（例如连接）来组合多模态子网络。最近的方法使用跨模态注意力机制来对齐来自不同模态的数据，同时保留所有模态的子网络。为了减少计算负担，一些工作通过共享参数和私有化规范化层来保持特定模式。相对于这些工作，本研究设计了迭代独立层归一化模块，用于多模态融合和交互。

### 相关工作部分详细解读

在图像描述领域，常见的方法是编码器-解码器框架。这些方法通常依赖于单一信息流，其核心是通过增加模型复杂性来改进架构。然而，这种方法存在限制，尤其是在处理多模态数据时。

最近的研究试图通过引入额外的视觉语义元素（如概念、属性和标签）来强化这些模型，但这些元素很难与原始的视觉特征有效对齐。这表明，单纯增加视觉语义元素可能不足以解决视觉和文本之间的语义差异问题。

泛观分割任务提供了一种可能的解决方案。这种方法旨在通过结合实例分割和语义分割来提供更丰富的视觉信息。例如，HIP系统通过层次结构解析来整合不同层次的特征，有助于在图像描述中更精确地表示视觉内容。

然而，多模态融合仍然是一个挑战，早期的融合策略（如简单的连接操作）往往无法有效地整合来自不同模态的信息。最近的方法采用跨模态注意力机制来改善这一点，但这通常需要保持所有模态的子网络，增加了计算复杂性。

迭代独立层归一化（IILN）的提出，旨在通过共享参数和私有化规范化层来更有效地融合和交互多模态数据。这种方法不仅减轻了计算负担，还保持了每个模态特定的信息，从而在保留各自特性的同时实现有效的信息融合。

这种新颖的融合方法对于提升图像描述模型的性能至关重要。它不仅提高了视觉信息的利用率，还增强了模型在处理多模态输入时的灵活性和效率。这些进展不仅对图像描述任务有着直接的影响，也对整个计算机视觉和自然语言处理领域的多模态研究具有重要意义。通过这些创新，研究者能够更有效地整合和利用来自不同来源的信息，从而推动深度学习技术在理解和生成自然语言描述方面的进步。


### Preliminaries部分翻译（中文）

本部分提供图像描述问题的定义。给定一张图像`I`，可以用一个句子`A`来描述，其中$A = {w_1, w_2, ..., w_L}$由`L`个词组成。令`V`表示由离线视觉特征提取器从图像`I`中提取的网格视觉特征，$V = {v_1, v_2, ..., v_N}$由`N`个网格组成，每个$v_i ∈ R^{D_v}$。与大多数现有的字幕系统类似，我们的工作基于编码器-解码器Transformer，它将网格特征`V`编码为一系列连续表示`Z`，然后与先前生成的单词配对来生成输出$y_t$。模型在每个时间步以自回归的方式产生句子中的一个词。这个标准范式可以表述为：


$y_t = \mathcal{F}_l(E_v(V), w_0, w_1, ..., w_{t-1})$

其中$E_v$是视觉编码器，$F_l$是语言解码器，$w_0$是起始符号。

#### Transformer架构
Transformer是一个序列转换模型。为了处理2D输入，我们需要将它们转换为一系列1D标记，如下所示：


$U' = Flatten(Pool(U))$  (2)


其中`U`是视觉特征（原始网格特征$O ∈ R^{H×W×D_v}$或分割特征`S`（将在第4.1节讨论）），$U' = {u'_1, u'_2, ..., u'_N}$是输入视觉特征序列（例如`V`），Pool是AdaptiveAvgPool2d，输出尺寸是$H' × W'$。然后我们使用线性投影将每个标记映射到$R^{d_{model}}$，如下所示：

$X = LN(σ(W_1U' + b_1)))$  (3)


其中`σ(·)`是ReLU激活函数，`LN`是层归一化，$X = {x_1, x_2, ..., x_N}$包含`N(N = H' × W')`个标记。然后，由`N_e`个Transformer层的堆栈组成的编码器用于将`X`映射到`Z`。每个Transformer层有两个子层：多头自注意力（MHSA）和逐位置前馈（PWFF）网络。我们将一个Transformer层表示为：

$\begin{aligned}M &= LN(MHSA(Z^l) + Z^l)\\Z^{l+1} &=LN(PWFF(M) + M)\end{aligned}$  (4)

其中`LN`是层归一化。解码器由一系列`N_d`个Transformer层组成，每个层在MHSA和PWFF之间插入第三个子层，该子层将编码器的输出和MHSA的输出作为输入。

### Preliminaries部分详细解读

在此部分，文章首先定义了图像描述问题。主要内容包括：

1. **问题定义**：对于给定的图像`I`，使用句子`A`进行描述，其中`A`由`L`个词组成。这要求将图像转换为一系列连续的表示形式。

2. **视觉特征**：使用网格视觉特征`V`，这些特征由离线视觉特征提取器从图像中提取，并包含`N`个网格，每个网格用`D_v`维空间中的向量表示。

3. **模型架构**：基于编码器-解码器架构的Transformer，该架构包括视觉编码器`E_v`和语言解码器`F_l`。模型在每个时间步自回归地产生句子中的一个词。

4. **Transformer架构**：为了处理2D视觉输入，它们被转换为1D标记序列。这包括应用自适应平均池化和线性投影，以及ReLU激活和层归一化。

5. **编码器和解码器层**：编码器由多个Transformer层组成，每层包含多头自注意力和逐位置前馈网络。解码器则在这两个子层之间添加了一个额外的子层，用于融合编码器的输出。

#### 公式解读
1. **公式(1)**：$y_t = \mathcal{F}_l(E_v(V), w_0, w_1, ..., w_{t-1})$ 表示在时间步`t`，解码器`F_l`基于视觉编码器`E_v`的输出和之前生成的单词预测下一个词。

2. **公式(2)**：$U' = Flatten(Pool(U))$描述了将2D视觉特征`U`转换为1D标记序列的过程，其中使用了平均池化和扁平化操作。

3. **公式(3)**：$X = LN(σ(W_1U' + b_1)))$ 进一步处理输入序列，通过线性变换和激活函数将其映射到模型的维度空间。

4. **公式(4)**：$\begin{aligned} M &= LN(MHSA(Z^l) + Z^l)\\ Z^{l+1} &= LN(PWFF(M) + M) \end{aligned}$描述了Transformer层的工作原理，包括多头自注意力和逐位置前馈网络，以及层归一化的应用。

以上分析提供了对模型架构和其工作原理的深入理解，展现了如何将视觉信息转换为语言描述的关键步骤和技术。




### 公式解读扩充

1. **公式(1)的深入解析**:
   $y_t = \mathcal{F}_l(E_v(V), w_0, w_1, ..., w_{t-1})$
   - `y_t`：在时间步`t`的输出词。
   - $\mathcal{F}_l$：语言解码器，负责生成描述文本。
   - `E_v(V)`：视觉编码器的输出，其中`V`是图像的视觉特征。
   - `w_0, w_1, ..., w_{t-1}`：到时间步`t-1`为止生成的单词序列。
   - 该公式表明，解码器在每一步使用视觉信息和之前生成的单词来预测下一个词。

2. **公式(2)的深入解析**:
   $U' = Flatten(Pool(U))$
   - `U`：原始的2D视觉特征矩阵。
   - `Pool`：自适应平均池化操作，降低特征的空间维度。
   - `Flatten`：扁平化操作，将2D特征转换为1D序列。
   - `U'`：转换后的1D视觉特征序列。
   - 这一步是将2D图像特征转换为适用于Transformer模型的1D序列的关键步骤。

3. **公式(3)的深入解析**:
   $X = LN(σ(W_1U' + b_1)))$
   - `σ(·)`：ReLU激活函数，增加非线性。
   - `W_1`和`b_1`：线性层的权重和偏置。
   - `LN`：层归一化，有助于训练过程的稳定性。
   - `X`：处理后的标记序列，作为Transformer模型的输入。
   - 该公式实现了对1D视觉特征序列的进一步处理，以便更好地融入模型架构。

4. **公式(4)的深入解析**:
   $\begin{aligned}   M &= LN(MHSA(Z^l) + Z^l)\\   Z^{l+1} &=LN(PWFF(M) + M)   \end{aligned}$
   - `MHSA`：多头自注意力机制，用于捕捉序列内不同位置间的关系。
   - `PWFF`：逐位置前馈网络，对特征进行进一步转换。
   - `LN`：层归一化，用于稳定训练。
   - `Z^l`：第`l`层的输入。
   - `Z^{l+1}`：第`l+1`层的输出。
   - `M`：注意力机制和原输入的残差连接后的中间结果。
   - 这一公式描述了Transformer层的内部工作机制，它通过结合注意力和前馈网络，有效地处理和转换特征。

以上分析提供了对模型架构中各个变量、步骤与计算细节的深入理解，揭示了如何将视觉信息转换为自然语言描述的关键技术。





### Method部分翻译（中文）

#### 4.1. 分割特征
全景分割图包含每个像素的语义类别信息和区分实例信息。因此，全景分割图可以被视为高级视觉语义线索，并提供粗粒度上下文。为了简单有效地适配网格特征，我们只从全景分割网络的语义分割头中提取语义分割图，然后将其转换为语义特征向量`S`，其中`S ∈ R^{H×W×C}`。每个`S`的维度是一个位图，表示一个语义类。在此基础上，我们的方法可以表示为：

$y_t = \mathcal{F}_l(E_v(V, S), w_0, w_1, ..., w_{t-1})$ (5)

#### 4.2. 网格与分割的融合
这部分展示如何在Transformer中整合两种输入表示。我们首先研究了一种融合策略VSA，然后介绍我们的融合方法IILN。

##### 通过普通自注意力的融合
给定网格输入序列`X_v`和分割输入序列`X_s`，首先分别使用Transformer层对它们进行编码，然后通过元素级求和将它们整合成一个序列`Z_{vs}`。为了交换信息并形成一个共同的表示，使用Transformer层对`Z_{vs}`进行编码。该过程可以表示为：


$\begin{aligned}Z_v^{l+1} &= Transformer(Z_v^l) & \text{if } l < L_f; \\Z_s^{l+1} &= Transformer(Z_s^l) & \text{if } l < L_f; \\Z_{vs}^{l+1} &= Transformer(Z_v^l + Z_s^l) & \text{if } l = L_f; \\Z_{vs}^{l+1} &= Transformer(Z_{vs}^l) & \text{otherwise}.\end{aligned}$  (6)

##### 通过迭代独立层归一化的融合
我们提出迭代独立层归一化（IILN）来克服上述问题。Transformer编码器层装备了IILN。我们首先共享MHSA层和PWFF层的参数，然后采用一个共同的LN层来获得包含两个表示的共同信息的单一分布。


$\begin{aligned}M_v &= LN(MHSA(Z_v^l; \theta_{vs}) + Z_v^l; \alpha_{vs}, \beta_{vs}) \\M_s &= LN(MHSA(Z_s^l; \theta_{vs}) + Z_s^l; alpha_{vs}, \beta_{vs})\end{aligned}$  (7)


然后应用两个私有LN层，将单一分布转换为两个模式特定的分布，这些分布融合了每个表示的私有信息和共同信息。


$\begin{aligned}M_v &= LN(M_v + Z_v; \alpha_v, \beta_v) \\M_s &= LN(M_s + Z_s; \alpha_s, \beta_s)\end{aligned}$  (8)


最后，应用PWFF和两个私有LN进一步增强两种表示。


$\begin{aligned}Z_v^{l+1} &= LN(PWFF(M_v; \theta_{vs}) + M_v; \alpha_v, \beta_v) \\Z_s^{l+1} &= LN(PWFF(M_s; \theta_{vs}) + M_s; \alpha_s, \beta_s)\end{aligned}$  (9)


我们还在IILN上应用适当次数的迭代T，以将更多信息融入到每个表示中。在IILN之后，两个表示的分布将更接近，同时保持模态特定信息。

### Method部分详细解读

#### 4.1. 分割特征
- **全景分割图**：全景分割图提供了像素级的语义类别和实例信息，是一个重要的视觉语义线索。
- **语义特征向量**：通过提取全景分割网络的语义分割头部分，转换为语义特征向量`S`。
- **方程(5)解读**：

  $y_t = \mathcal{F}_l(E_v(V, S), w_0, w_1, ..., w_{t-1})$  (5)

  - $y_t$：在时间步`t`的输出词。
  - $\mathcal{F}_l$：语言解码器。
  - $E_v(V, S)$：结合了传统视觉特征`V`和分割特征`S`的视觉编码器输出。
  - 此方程展示了如何结合传统视觉特征和分割特征来增强模型的预测能力。

#### 4.2. 网格与分割的融合
- **普通自注意力的融合**：
  - **方程(6)解读**：

    $\begin{aligned}    Z_v^{l+1} &= Transformer(Z_v^l) & \text{if } l < L_f; \\    Z_s^{l+1} &= Transformer(Z_s^l) & \text{if } l < L_f; \\    Z_{vs}^{l+1} &= Transformer(Z_v^l + Z_s^l) & \text{if } l = L_f; \\    Z_{vs}^{l+1} &= Transformer(Z_{vs}^l) & \text{otherwise}.    \end{aligned}$  (6)  
  - $Z_v^{l+1}, Z_s^{l+1}$：分别是网格和分割信息通过Transformer层处理后的输出。
  - $Z_{vs}^{l+1}$：是融合了网格和分割信息后的输出。

- **迭代独立层归一化的融合**（IILN）：
  - **方程(7)解读**：
   
    $\begin{aligned}    M_v &= LN(MHSA(Z_v^l; \theta_{vs}) + Z_v^l; \alpha_{vs},\beta_{vs}) \\    M_s &= LN(MHSA(Z_s^l; \theta_{vs}) + Z_s^l; \alpha_{vs}, beta_{vs})    \end{aligned}$ (7)
    
  - `M_v`, `M_s`：分别是网格和分割信息经过多头自注意力和层归一化后的中间表示。
  
  - **方程(8)解读**：

    $\begin{aligned}    M_v &= LN(M_v + Z_v; \alpha_v, \beta_v) \\    M_s &= LN(M_s + Z_s; \alpha_s, \beta_s)    \end{aligned}$  (8)

  - 该方程进一步加强了两种表示的特定模态信息。

  - **方程(9)解读**：

    $\begin{aligned}    Z_v^{l+1} &= LN(PWFF(M_v; \theta_{vs}) + M_v; \alpha_v, \beta_v) \\    Z_s^{l+1} &= LN(PWFF(M_s; \theta_{vs}) + M_s; \alpha_s, \beta_s)    \end{aligned}$  (9)

  - $Z_v^{l+1}, Z_s^{l+1}$：分别是经过逐位置前馈网络处理后的网格和分割信息。

- **IILN的目的**：通过迭代处理，使两种表示的分布更接近，同时保留各自的模态特定信息。

以上解读提供了对方法部分至4.3节之前内容的详细理解，展示了如何有效融合网格特



我现在开始翻译您所提供的论文“DIFNet: Boosting Visual Information Flow for Image Captioning”中第4.3节“Extra Skip Connection”到第5节“Experiments”之间的部分，并对这部分内容进行详细解读。这将包括对公式和变量的解析。

### 翻译（第4.3节至第5节）

#### 4.3. 额外的跳跃连接
由于注意力机制的特性，它可能过滤掉一些不太明显但潜在有用的信息。为了增强编码器和解码器之间以及它们内部的信息流，并保护一些脆弱的视觉信息不被注意力层过滤掉，我们在它们上添加了额外的跳跃连接。类似于递归跳跃连接，我们首先在变换器的多头自注意力（MHSA）上添加了一个额外的跳跃连接，其可以表示为：

$M = LN(LN(MHSA(Z)+Z)+Z)$

这可以帮助整合可能被MHSA过滤掉的更多信息。此外，通过IILN层获得的信息在经过多个变换器层后可能无法有效保留。我们从第Lf个编码器层的输出添加跳跃连接到编码器的输出，以强制各种流中的有用信息直接流向解码器：

$Z = Z_{vs}^{N_{e}} + Z_{v}^{L_{f}} + Z_{s}^{L_{f}}$

结合IILN和跳跃连接后，编码器中的视觉信息被最大化。

#### 4.4. 训练细节
我们遵循图像字幕任务中标准的两阶段训练策略：首先用交叉熵损失预训练模型，然后用强化学习进行微调。在强化学习阶段，我们遵循M2 Transformer的训练方法，整个模型优化以CIDEr奖励进行，这是图像字幕的最受欢迎的度量指标。

#### 5. 实验
在这一节中，我们详细介绍了我们的实验设置，包括数据集、评估指标以及实验结果。我们还对比了我们的模型与其他最先进方法的性能，并进行了详尽的消融研究来分析不同组件的影响。

### 详细解读

#### 额外跳跃连接
- **目的**：强化信息流并保护重要但不明显的视觉信息。
- **方法**：在多头自注意力（MHSA）上添加额外的跳跃连接，确保即使在复杂的注意力处理中，也能保留有用的信息。
- **表达式解析**：`M = LN(LN(MHSA(Z)+Z)+Z)`，其中MHSA代表多头自注意力机制，LN代表层归一化。这个表达式通过添加额外的层归一化和跳跃连接，将信息集成到模型中。

#### 训练细节
- **两阶段训练策略**：先用交叉熵损失进行预训练，再用基于CIDEr奖励的强化学习进行微调，这有助于模型更好地调整其生成的字幕以适应评估指标。

#### 实验设置
- **数据集和评估指标**：使用流行的MS-COCO数据集进行评估，并采用多种评估指标如BLEU、METEOR、ROUGE、CIDEr和SPICE。
- **性能对比**：与现有的先